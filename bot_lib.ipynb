{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b51e0e-8190-4b02-86be-64665a7f878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain --quiet\n",
    "%pip install numpy --quiet\n",
    "%pip install boto3 --quiet\n",
    "%pip install faiss-gpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cf846e-2d8a-4711-a002-c1420582e285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd08f99-0b79-4ac1-9282-16be296eefcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## customize and create LLM\n",
    "\n",
    "def get_llm():\n",
    "    \n",
    "    model_kwargs = { #AI21\n",
    "        \"maxTokens\": 2000, \n",
    "        \"temperature\": 0, \n",
    "        \"topP\": 0.5, \n",
    "        \"stopSequences\": [], \n",
    "        \"countPenalty\": {\"scale\": 0 }, \n",
    "        \"presencePenalty\": {\"scale\": 0 }, \n",
    "        \"frequencyPenalty\": {\"scale\": 0 } \n",
    "    }\n",
    "    \n",
    "    llm = Bedrock(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"), #sets the profile name to use for AWS credentials \n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"), #sets the region name \n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"), #sets the endpoint URL \n",
    "        model_id=\"ai21.j2-ultra-v1\", #set the foundation model\n",
    "        model_kwargs=model_kwargs) #configure the properties for Claude\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288a1ae3-d7d4-4073-8357-205ae92aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the helper that returns an in-memory vector index to be used in application\n",
    "\n",
    "def get_index(file):\n",
    "    \n",
    "    embeddings = BedrockEmbeddings(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"), \n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"), \n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"), \n",
    "    ) #create a Titan Embeddings client\n",
    "\n",
    "    loader = PyPDFLoader(file) #load the pdf file\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        separators=[\"\\n\\n\", \"â€¢\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=100 #number of characters that can overlap with previous chunk\n",
    "    )\n",
    "    \n",
    "    index_creator = VectorstoreIndexCreator( #create a vector store factory\n",
    "        vectorstore_cls=FAISS, #use an in-memory vector store for demo purposes\n",
    "        embedding=embeddings, #use Titan embeddings\n",
    "        text_splitter=text_splitter, #use the recursive text splitter\n",
    "    )\n",
    "    \n",
    "    index_from_loader = index_creator.from_loaders([loader]) #create an vector store index from the loaded PDF\n",
    "    return index_from_loader #return the index to be cached by the client app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e46766b-5515-48fb-b04d-0c9d2ed6be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the helper that receive result from LLM\n",
    "\n",
    "def get_rag_response(index, question): #rag client function\n",
    "    llm = get_llm()\n",
    "    response_text = index.query(question=question, llm=llm)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fc6d5c-4229-43cf-a894-0ae9dce27e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Helpful classes\n",
    "\n",
    "belc = BedrockEmbeddings()\n",
    "\n",
    "class EmbedItem:\n",
    "    def __init__(self, text, position):\n",
    "        self.text = text\n",
    "        self.embedding = belc.embed_query(text)\n",
    "        self.position = position\n",
    "\n",
    "class ComparisonResult:\n",
    "    def __init__(self, text, similarity, position):\n",
    "        self.text = text\n",
    "        self.similarity = similarity\n",
    "        self.position = position\n",
    "        \n",
    "def calculate_similarity(a, b): \n",
    "    return dot(a, b) / (norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c89f58-b451-432a-bcb7-4a0e9e64b373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Main function that extracts key information from uploaded resume\n",
    "\n",
    "def run(n):\n",
    "    \n",
    "    candidates = []\n",
    "    input_text = \"Extract the technical skills which is only supported by the experiences listed in the resume, and the language, candidate name, sex, country and education of the resume. And filter out the skills without being supported by experiences.\"\n",
    "    \n",
    "    for i in range(1,n+1,1):\n",
    "        index = get_index(\"~/home/jovyan/GenerativeAIBot/Resume\"+str(i)+\".pdf\")\n",
    "        response_content = get_rag_response(index=index, question=input_text)\n",
    "        candidates.append(EmbedItem(response_content, i))\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7f2d38-0510-4ccb-bda1-9ff11c5717d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## returns the result of similarity match between resume and inputed job description\n",
    "\n",
    "def compare(job_description, items):\n",
    "    result = []\n",
    "    e1 = EmbedItem(job_description, 0)\n",
    "    cosine_comparisons = []\n",
    "    \n",
    "    for e2 in items:\n",
    "        similarity_score = calculate_similarity(e1.embedding, e2.embedding)\n",
    "        cosine_comparisons.append(ComparisonResult(e2.text, similarity_score, e2.position)) #save the comparisons to a list\n",
    "    \n",
    "    cosine_comparisons.sort(key=lambda x: x.similarity, reverse=True) # list the closest matches first\n",
    "    for c in cosine_comparisons:\n",
    "        result.append(str(c.similarity)+(\" - resume\"+str(c.position)))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671bd068-40a1-4c1e-b393-91fb409be0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4980325748478376 - resume3\n",
      "0.43871089659190554 - resume1\n",
      "0.27346247389725076 - resume2\n"
     ]
    }
   ],
   "source": [
    "# Manual testing of backend library    \n",
    "    \n",
    "input_text = \"knows java and have experience as a quality analyte\"\n",
    "items = run(3)\n",
    "response_content = compare(job_description=input_text, items=items)\n",
    "for line in response_content:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
